{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GlenM42/PCA/blob/main/DataMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKD-RNip24Um"
   },
   "source": [
    "# Problem 1: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:47:57.446237Z",
     "start_time": "2024-10-03T18:47:57.119296Z"
    },
    "id": "5n51MC6b2nzU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This is going to be our function to perform a PCA:\n",
    "def pca(data, n_components=None):\n",
    "  \"\"\"\n",
    "  :param data: numpy array of shape (n_samples, n_features)\n",
    "  :param n_components: number of principal components to return, optional\n",
    "  :return: transformed data, eigenvalues, eigenvectors\n",
    "  \"\"\"\n",
    "\n",
    "  # Step 1: Center the data\n",
    "  mean = np.mean(data, axis=0)\n",
    "  centered_data = data - mean\n",
    "\n",
    "  # Step 2: Compute the covariance matrix\n",
    "  cov_matrix = np.cov(centered_data, rowvar=False)\n",
    "\n",
    "  # Step 3: Solve the char polynomial\n",
    "  eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "  # Step 3.5: Find the char polynomial\n",
    "  char_poly = np.poly(eigenvalues)\n",
    "\n",
    "  # Step 4: Sort eigenvalues and eigenvectors in descending order of eigenvalues\n",
    "  sorted_ind = np.argsort(eigenvalues)[::-1]\n",
    "  sorted_eigenvalues = eigenvalues[sorted_ind]\n",
    "  sorted_eigenvectors = eigenvectors[:, sorted_ind]\n",
    "\n",
    "  # Step 5: Select the top n_comp if specified\n",
    "  if n_components is not None:\n",
    "    sorted_eigenvectors = sorted_eigenvectors[:, :n_components]\n",
    "\n",
    "  # Step 6: Calculate the explained variance\n",
    "  total_var = np.sum(sorted_eigenvalues)\n",
    "  explained_var = (sorted_eigenvalues / total_var) * 100\n",
    "\n",
    "  # Step 7: Transform the data using the selected eigenvectors\n",
    "  transformed_data = np.dot(centered_data, sorted_eigenvectors)\n",
    "\n",
    "  return {\n",
    "      \"mean\": mean,\n",
    "      'centered_data': centered_data,\n",
    "      'char_poly': char_poly,\n",
    "      'cov_matrix': cov_matrix,\n",
    "      'eigenvalues': sorted_eigenvalues,\n",
    "      'eigenvectors': sorted_eigenvectors,\n",
    "      'explained_variance': explained_var,\n",
    "      'transformed_data': transformed_data\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJubR6Ov4nd-"
   },
   "source": [
    "With this function defined, let's use it on our array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:48:01.283039Z",
     "start_time": "2024-10-03T18:48:01.278296Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMy3I-ol4rDm",
    "outputId": "50319deb-c93f-4017-960c-20ce1c6d1d9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      "[ 7.   5.  99.8]\n",
      "\n",
      "Centered_data: \n",
      "[[-2.  -2.  -4.8]\n",
      " [-1.  -1.  -1.8]\n",
      " [ 0.   0.   0.2]\n",
      " [ 1.   1.   2.2]\n",
      " [ 2.   2.   4.2]]\n",
      "\n",
      "Char_poly: \n",
      "[ 1.00000000e+00 -1.72000000e+01  5.00000000e-01 -3.03153062e-16]\n",
      "\n",
      "Cov_matrix: \n",
      "[[ 2.5  2.5  5.5]\n",
      " [ 2.5  2.5  5.5]\n",
      " [ 5.5  5.5 12.2]]\n",
      "\n",
      "Eigenvalues: \n",
      "[1.71708809e+01 2.91190651e-02 6.06306124e-16]\n",
      "\n",
      "Eigenvectors: \n",
      "[[-3.80779846e-01 -5.95824394e-01 -7.07106781e-01]\n",
      " [-3.80779846e-01 -5.95824394e-01  7.07106781e-01]\n",
      " [-8.42622939e-01  5.38504022e-01 -9.30532457e-15]]\n",
      "\n",
      "Explained_variance: \n",
      "[9.98307031e+01 1.69296890e-01 3.52503560e-15]\n",
      "\n",
      "Transformed_data: \n",
      "[[ 5.56770949e+00 -2.01521729e-01  3.58730602e-15]\n",
      " [ 2.27828098e+00  2.22341549e-01 -3.78954173e-15]\n",
      " [-1.68524588e-01  1.07700804e-01 -1.86106491e-15]\n",
      " [-2.61533016e+00 -6.93993973e-03  6.74119041e-17]\n",
      " [-5.06213573e+00 -1.21580684e-01  1.99588872e-15]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = pca(np.array([[5, 3, 95],\n",
    "                        [6, 4, 98],\n",
    "                        [7, 5, 100],\n",
    "                        [8, 6, 102],\n",
    "                        [9, 7, 104]]))\n",
    "\n",
    "# Output the results\n",
    "for key, value in results.items():\n",
    "  print(f\"{key.capitalize()}: \\n{value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8Du-H5T_cJx"
   },
   "source": [
    "# Problem 2: Binary Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:48:29.476103Z",
     "start_time": "2024-10-03T18:48:29.468564Z"
    },
    "id": "Bi784uqa8SP5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Function to calculate Gini index\n",
    "def gini(y):\n",
    "    counts = Counter(y)\n",
    "    impurity = 1 - sum((count / len(y))**2 for count in counts.values())\n",
    "    return impurity\n",
    "\n",
    "# Function to calculate Entropy\n",
    "def entropy(y):\n",
    "    counts = Counter(y)\n",
    "    impurity = -sum((count / len(y)) * np.log2(count / len(y)) for count in counts.values() if count != 0)\n",
    "    return impurity\n",
    "\n",
    "# Function to perform the binary split and return the best split point\n",
    "def best_split(X, y, feature_labels, method=\"gini\", attribute_type=\"continuous\"):\n",
    "    if method == \"gini\":\n",
    "        impurity_func = gini\n",
    "    elif method == \"entropy\":\n",
    "        impurity_func = entropy\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'gini' or 'entropy'\")\n",
    "\n",
    "    best_split_point = None\n",
    "    best_impurity = float('inf')\n",
    "    best_left = None\n",
    "    best_right = None\n",
    "    best_feature = None\n",
    "\n",
    "    for col in range(X.shape[1]):\n",
    "        values = X[:, col]\n",
    "        sorted_indices = np.argsort(values)\n",
    "        sorted_values, sorted_labels = values[sorted_indices], y[sorted_indices]\n",
    "        for i in range(1, len(sorted_values)):\n",
    "            split_point = (sorted_values[i - 1] + sorted_values[i]) / 2\n",
    "            left_labels = sorted_labels[:i]\n",
    "            right_labels = sorted_labels[i:]\n",
    "            left_impurity = impurity_func(left_labels)\n",
    "            right_impurity = impurity_func(right_labels)\n",
    "            impurity = (len(left_labels) / len(y)) * left_impurity + (len(right_labels) / len(y)) * right_impurity\n",
    "\n",
    "            if impurity < best_impurity:\n",
    "                best_impurity = impurity\n",
    "                best_split_point = split_point\n",
    "                best_left = (X[sorted_indices[:i], :], y[sorted_indices[:i]])\n",
    "                best_right = (X[sorted_indices[i:], :], y[sorted_indices[i:]])\n",
    "                best_feature = feature_labels[col]  # Track the best feature\n",
    "\n",
    "    return best_split_point, best_left, best_right, best_impurity, best_feature\n",
    "\n",
    "# Main function d_tree to perform the binary split and output important metrics\n",
    "def d_tree(X, y, feature_labels, method=\"gini\", attribute_type=\"continuous\"):\n",
    "    best_split_point, best_left, best_right, best_impurity, best_feature = best_split(X, y, feature_labels, method, attribute_type)\n",
    "\n",
    "    print(f\"Best Feature to Split On: {best_feature}\")\n",
    "    print(f\"Best Split Point: {best_split_point}\")\n",
    "    print(f\"Best Left Group Shape: {best_left[0].shape}\")\n",
    "    print(f\"Best Right Group Shape: {best_right[0].shape}\")\n",
    "    print(f\"Best Impurity (Gini/Entropy): {best_impurity}\")\n",
    "\n",
    "    print(\"\\nLeft Group Labels Distribution:\")\n",
    "    print(Counter(best_left[1]))\n",
    "\n",
    "    print(\"\\nRight Group Labels Distribution:\")\n",
    "    print(Counter(best_right[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmvI6negCEEu"
   },
   "source": [
    "With the functions defined, we can do the calculations on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZSEwYgMCKgC",
    "outputId": "968a4d80-0383-4558-ef79-704e01c21fea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Feature to Split On: Car Type_Luxury\n",
      "Best Split Point: 0.0\n",
      "Best Left Group Shape: (9, 9)\n",
      "Best Right Group Shape: (11, 9)\n",
      "Best Impurity (Gini/Entropy): 0.2417233428068324\n",
      "\n",
      "Left Group Labels Distribution:\n",
      "Counter({'C0': 9})\n",
      "\n",
      "Right Group Labels Distribution:\n",
      "Counter({'C1': 10, 'C0': 1})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Example dataset\n",
    "data = [\n",
    "    [1, 'M', 'Family', 'Small', 'C0'],\n",
    "    [2, 'M', 'Sports', 'Medium', 'C0'],\n",
    "    [3, 'M', 'Sports', 'Medium', 'C0'],\n",
    "    [4, 'M', 'Sports', 'Large', 'C0'],\n",
    "    [5, 'M', 'Sports', 'Extra Large', 'C0'],\n",
    "    [6, 'M', 'Sports', 'Extra Large', 'C0'],\n",
    "    [7, 'F', 'Sports', 'Small', 'C0'],\n",
    "    [8, 'F', 'Sports', 'Small', 'C0'],\n",
    "    [9, 'F', 'Sports', 'Medium', 'C0'],\n",
    "    [10, 'M', 'Luxury', 'Large', 'C0'],\n",
    "    [11, 'M', 'Family', 'Large', 'C1'],\n",
    "    [12, 'M', 'Family', 'Extra Large', 'C1'],\n",
    "    [13, 'M', 'Family', 'Medium', 'C1'],\n",
    "    [14, 'M', 'Family', 'Extra Large', 'C1'],\n",
    "    [15, 'F', 'Luxury', 'Small', 'C1'],\n",
    "    [16, 'F', 'Luxury', 'Medium', 'C1'],\n",
    "    [17, 'F', 'Luxury', 'Medium', 'C1'],\n",
    "    [18, 'F', 'Luxury', 'Medium', 'C1'],\n",
    "    [19, 'F', 'Luxury', 'Medium', 'C1'],\n",
    "    [20, 'F', 'Luxury', 'Large', 'C1']\n",
    "]\n",
    "feature_labels = ['Gender', 'Car Type', 'Shirt Size']\n",
    "\n",
    "# Convert the data into a NumPy array and split into features (X) and labels (y)\n",
    "data = np.array(data)\n",
    "X_raw = data[:, 1:4]  # Features\n",
    "y = data[:, -1]       # Labels\n",
    "\n",
    "# One-hot encoding for categorical features (Gender, Car Type, Shirt Size)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "X_encoded = encoder.fit_transform(X_raw)\n",
    "encoded_feature_labels = encoder.get_feature_names_out(feature_labels)\n",
    "\n",
    "# Perform the decision tree binary split using Gini for the encoded categorical data\n",
    "d_tree(\n",
    "    X=X_encoded,\n",
    "    y=y,\n",
    "    feature_labels=encoded_feature_labels,\n",
    "    method=\"entropy\",\n",
    "    attribute_type=\"categorical\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMA7Tq7GTpEd1JE8jKyzBk5",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
